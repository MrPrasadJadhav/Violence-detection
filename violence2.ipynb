{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3TvV1gQcd5SI",
        "outputId": "5c3616b5-974c-43eb-f7da-e9794bdd4668"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "pUMaWwNHdhcc",
        "outputId": "13d2265e-7df0-46d2-d552-8c751bcc0be9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 95%|█████████▌| 475/500 [59:53<02:37,  6.30s/it]"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import InceptionV3\n",
        "from tensorflow.keras.models import Model\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Load the pretrained InceptionV3 model\n",
        "def load_pretrained_model():\n",
        "    base_model = InceptionV3(include_top=False, weights='imagenet', pooling='avg')\n",
        "    model = Model(inputs=base_model.input, outputs=base_model.output)\n",
        "    return model\n",
        "\n",
        "pretrained_model = load_pretrained_model()\n",
        "\n",
        "# Extract frames from a video\n",
        "def extract_video_frames(video_path, sequence_length=16, image_width=299, image_height=299):\n",
        "    frames_list = []\n",
        "    video_reader = cv2.VideoCapture(video_path)\n",
        "    video_frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    skip_frames_window = max(int(video_frames_count / sequence_length), 1)\n",
        "\n",
        "    for frame_counter in range(sequence_length):\n",
        "        video_reader.set(cv2.CAP_PROP_POS_FRAMES, frame_counter * skip_frames_window)\n",
        "        success, frame = video_reader.read()\n",
        "        if not success:\n",
        "            break\n",
        "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        resized_frame = cv2.resize(frame_rgb, (image_height, image_width))\n",
        "        frames_list.append(resized_frame)\n",
        "\n",
        "    video_reader.release()\n",
        "    return frames_list\n",
        "\n",
        "# Extract features from frames using the pretrained model\n",
        "def extract_frame_features(frame, pretrained_model):\n",
        "    img = np.expand_dims(frame, axis=0)\n",
        "    img = tf.keras.applications.inception_v3.preprocess_input(img)\n",
        "    feature_vector = pretrained_model.predict(img, verbose=0)\n",
        "    return feature_vector\n",
        "\n",
        "# Extract features from videos\n",
        "def extract_features_from_videos(video_paths, pretrained_model):\n",
        "    all_video_features = []\n",
        "    for video_path in tqdm(video_paths):\n",
        "        frames_list = extract_video_frames(video_path)\n",
        "        frames_features = [extract_frame_features(frame, pretrained_model) for frame in frames_list]\n",
        "        all_video_features.append(frames_features)\n",
        "    return np.array(all_video_features)\n",
        "\n",
        "# Define violence and non-violence directories\n",
        "violence_dir = '/content/drive/MyDrive/archive (7)/Real Life Violence Dataset/Violence'\n",
        "nonviolence_dir = '/content/drive/MyDrive/archive (7)/Real Life Violence Dataset/NonViolence'\n",
        "\n",
        "# Create paths to individual videos\n",
        "violence_path = [os.path.join(violence_dir, name) for name in os.listdir(violence_dir)]\n",
        "nonviolence_path = [os.path.join(nonviolence_dir, name) for name in os.listdir(nonviolence_dir)]\n",
        "\n",
        "# Extract features from videos\n",
        "violence_features = extract_features_from_videos(violence_path[:500], pretrained_model)\n",
        "non_violence_features = extract_features_from_videos(nonviolence_path[:500], pretrained_model)\n",
        "\n",
        "# Save extracted features\n",
        "np.save('/content/drive/MyDrive/archive (7)/real life violence situations/Violence_features.npy', violence_features)\n",
        "np.save('/content/drive/MyDrive/archive (7)/real life violence situations/NonViolence_features.npy', non_violence_features)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "bfEK8qBwxblq",
        "outputId": "3ac2b179-ae99-48f3-a958-39233ba87d1a"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'np' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-0c8f234e8e83>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Load features and labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mviolence_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/archive (7)/real life violence situations/Violence_features.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mnon_violence_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/archive (7)/real life violence situations/NonViolence_features.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.layers import Input, LSTM, Bidirectional, BatchNormalization, Dropout, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load features and labels\n",
        "violence_features = np.load('/content/drive/MyDrive/archive (7)/real life violence situations/Violence_features.npy')\n",
        "non_violence_features = np.load('/content/drive/MyDrive/archive (7)/real life violence situations/NonViolence_features.npy')\n",
        "\n",
        "# Creating labels\n",
        "violence_labels = np.ones(len(violence_features))\n",
        "non_violence_labels = np.zeros(len(non_violence_features))\n",
        "\n",
        "# Combining features and labels\n",
        "X = np.concatenate([violence_features, non_violence_features], axis=0)\n",
        "y = np.concatenate([violence_labels, non_violence_labels], axis=0)\n",
        "\n",
        "# Splitting data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=32)\n",
        "\n",
        "# Reshaping data for LSTM input\n",
        "X_train_reshaped = X_train.reshape((X_train.shape[0], 16, 2048))\n",
        "X_test_reshaped = X_test.reshape((X_test.shape[0], 16, 2048))\n",
        "\n",
        "# Building the LSTM model\n",
        "inputs = Input(shape=(16, 2048))\n",
        "x = Bidirectional(LSTM(200, return_sequences=True))(inputs)\n",
        "x = BatchNormalization()(x)\n",
        "x = Dropout(0.3)(x)\n",
        "x = Bidirectional(LSTM(100))(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Dropout(0.3)(x)\n",
        "x = Dense(200, activation='relu')(x)\n",
        "outputs = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "model = Model(inputs=inputs, outputs=outputs)\n",
        "model.summary()\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=3e-4), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train_reshaped, y_train, epochs=10, batch_size=2, validation_data=(X_test_reshaped, y_test))\n",
        "\n",
        "# Save the trained model\n",
        "model.save('/content/drive/MyDrive/archive (7)/real life violence situations/violence_detection_model_sgd.h5')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Bf6V3HW7gwe"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "\n",
        "# Load the trained model\n",
        "model = load_model('/content/drive/MyDrive/archive (7)/real life violence situations/violence_detection_model_sgd.h5')\n",
        "\n",
        "def extract_video_frames_for_prediction(video_path, sequence_length=16, image_width=299, image_height=299):\n",
        "    frames_list = []\n",
        "    video_reader = cv2.VideoCapture(video_path)\n",
        "    video_frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    skip_frames_window = max(int(video_frames_count / sequence_length), 1)\n",
        "\n",
        "    for frame_counter in range(sequence_length):\n",
        "        video_reader.set(cv2.CAP_PROP_POS_FRAMES, frame_counter * skip_frames_window)\n",
        "        success, frame = video_reader.read()\n",
        "        if not success:\n",
        "            break\n",
        "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        resized_frame = cv2.resize(frame_rgb, (image_height, image_width))\n",
        "        frames_list.append(resized_frame)\n",
        "\n",
        "    video_reader.release()\n",
        "    return frames_list\n",
        "\n",
        "def extract_frame_features_for_prediction(frame, pretrained_model):\n",
        "    img = np.expand_dims(frame, axis=0)\n",
        "    img = tf.keras.applications.inception_v3.preprocess_input(img)\n",
        "    feature_vector = pretrained_model.predict(img, verbose=0)\n",
        "    return feature_vector\n",
        "\n",
        "def predict_violence_in_video(video_path, pretrained_model, model):\n",
        "    frames = extract_video_frames_for_prediction(video_path)\n",
        "    frames_features = [extract_frame_features_for_prediction(frame, pretrained_model) for frame in frames]\n",
        "    frames_features = np.array(frames_features).reshape((1, 16, 2048))\n",
        "\n",
        "    prediction = model.predict(frames_features)\n",
        "    return prediction[0][0]\n",
        "\n",
        "# Upload a video file\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Assuming the pretrained InceptionV3 model is available\n",
        "pretrained_model = load_pretrained_model()\n",
        "\n",
        "# Predict violence for each uploaded video\n",
        "for video_filename in uploaded.keys():\n",
        "    print(f'Processing video: {video_filename}')\n",
        "    prediction = predict_violence_in_video(video_filename, pretrained_model, model)\n",
        "    print(f'Prediction score for {video_filename}: {prediction}')\n",
        "    if prediction > 0.5:\n",
        "        print(f'Violence detected in {video_filename}')\n",
        "    else:\n",
        "        print(f'No violence detected in {video_filename}')\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}